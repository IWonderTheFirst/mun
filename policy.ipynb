{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11678198,"sourceType":"datasetVersion","datasetId":7329647},{"sourceId":11963204,"sourceType":"datasetVersion","datasetId":7522543},{"sourceId":12021011,"sourceType":"datasetVersion","datasetId":7562970}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing dependencies","metadata":{}},{"cell_type":"code","source":"!pip install node2vec","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:28:39.174623Z","iopub.execute_input":"2025-07-12T01:28:39.174823Z","iopub.status.idle":"2025-07-12T01:28:50.954863Z","shell.execute_reply.started":"2025-07-12T01:28:39.174785Z","shell.execute_reply":"2025-07-12T01:28:50.954127Z"}},"outputs":[{"name":"stdout","text":"Collecting node2vec\n  Downloading node2vec-0.5.0-py3-none-any.whl.metadata (849 bytes)\nRequirement already satisfied: gensim<5.0.0,>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from node2vec) (4.3.3)\nRequirement already satisfied: joblib<2.0.0,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from node2vec) (1.5.0)\nRequirement already satisfied: networkx<4.0.0,>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from node2vec) (3.4.2)\nRequirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from node2vec) (1.26.4)\nRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from node2vec) (4.67.1)\nCollecting scipy<1.14.0,>=1.7.0 (from gensim<5.0.0,>=4.3.0->node2vec)\n  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim<5.0.0,>=4.3.0->node2vec) (7.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.24.0->node2vec) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.24.0->node2vec) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.24.0->node2vec) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.24.0->node2vec) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.24.0->node2vec) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.24.0->node2vec) (2.4.1)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.0->node2vec) (1.17.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0,>=1.24.0->node2vec) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0,>=1.24.0->node2vec) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0,>=1.24.0->node2vec) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0.0,>=1.24.0->node2vec) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0.0,>=1.24.0->node2vec) (2024.2.0)\nDownloading node2vec-0.5.0-py3-none-any.whl (7.2 kB)\nDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scipy, node2vec\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.15.2\n    Uninstalling scipy-1.15.2:\n      Successfully uninstalled scipy-1.15.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed node2vec-0.5.0 scipy-1.13.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport random\n\n#Data\nimport numpy as np\nimport pandas as pd\nimport re\nimport json\nimport math\n\n#Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport networkx as nx\nfrom tqdm import tqdm\n\n# Models\nfrom kaggle_secrets import UserSecretsClient\nfrom openai import AzureOpenAI\nfrom openai import OpenAI\nfrom sentence_transformers import SentenceTransformer\nfrom node2vec import Node2Vec\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataloader, Dataset\nimport umap\n\n#Metrics\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\nfrom sklearn.preprocessing import StandardScaler\nfrom collections import defaultdict\n\n#Transformations\nfrom sklearn.preprocessing import normalize\nimport networkx as nx\nfrom itertools import combinations\nfrom sklearn.manifold import TSNE\nfrom scipy.spatial.distance import pdist, squareform\nfrom itertools import combinations\nfrom collections import Counter\nimport ast","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:28:50.956832Z","iopub.execute_input":"2025-07-12T01:28:50.957083Z","iopub.status.idle":"2025-07-12T01:30:10.927529Z","shell.execute_reply.started":"2025-07-12T01:28:50.957058Z","shell.execute_reply":"2025-07-12T01:30:10.926860Z"}},"outputs":[{"name":"stderr","text":"2025-07-12 01:29:08.276616: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752283748.455699      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752283748.504684      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Policy Embeddings\nIn this notebook, I vectorized each nation's political stance by:\n- cleaning text\n- embedding country speeches with semantic segmentation and averaging\n- graph based knowledge embedding utilzied country information such as: political bloc, voting record\n- combining vectors","metadata":{}},{"cell_type":"markdown","source":"# Cleaning Text","metadata":{}},{"cell_type":"code","source":"# Function for cleaning data\n\ndef clean_text(text: str) -> str:\n    \"\"\"Clean text by removing country names while preserving stopwords\"\"\"\n    print(\"Cleaning text\")\n    country_names = [\n        \"Afghanistan\", \"Albania\", \"Algeria\", \"Andorra\", \"Angola\", \n        \"Antigua\", \"Argentina\", \"Armenia\", \"Australia\", \"Austria\",\n        \"Azerbaijan\", \"Bahamas\", \"Bahrain\", \"Bangladesh\", \"Barbados\",\n        \"Belarus\", \"Belgium\", \"Belize\", \"Benin\", \"Bhutan\",\n        \"Bolivia\", \"Bosnia\", \"Botswana\", \"Brazil\", \"Brunei\",\n        \"Bulgaria\", \"Burkina\", \"Burundi\", \"Cambodia\", \"Cameroon\",\n        \"Canada\", \"Cape Verde\", \"Central African Republic\", \"Chad\", \"Chile\",\n        \"China\", \"Colombia\", \"Comoros\", \"Congo\", \"Costa Rica\",\n        \"Croatia\", \"Cuba\", \"Cyprus\", \"Czech Republic\", \"Denmark\",\n        \"Djibouti\", \"Dominica\", \"Dominican Republic\", \"Ecuador\", \"Egypt\",\n        \"El Salvador\", \"Equatorial Guinea\", \"Eritrea\", \"Estonia\", \"Eswatini\",\n        \"Ethiopia\", \"Fiji\", \"Finland\", \"France\", \"Gabon\",\n        \"Gambia\", \"Georgia\", \"Germany\", \"Ghana\", \"Greece\",\n        \"Grenada\", \"Guatemala\", \"Guinea\", \"Guinea-Bissau\", \"Guyana\",\n        \"Haiti\", \"Honduras\", \"Hungary\", \"Iceland\", \"India\",\n        \"Indonesia\", \"Iran\", \"Iraq\", \"Ireland\", \"Israel\",\n        \"Italy\", \"Ivory Coast\", \"Jamaica\", \"Japan\", \"Jordan\",\n        \"Kazakhstan\", \"Kenya\", \"Kiribati\", \"Korea\", \"Kosovo\",\n        \"Kuwait\", \"Kyrgyzstan\", \"Laos\", \"Latvia\", \"Lebanon\",\n        \"Lesotho\", \"Liberia\", \"Libya\", \"Liechtenstein\", \"Lithuania\",\n        \"Luxembourg\", \"Madagascar\", \"Malawi\", \"Malaysia\", \"Maldives\",\n        \"Mali\", \"Malta\", \"Marshall Islands\", \"Mauritania\", \"Mauritius\",\n        \"Mexico\", \"Micronesia\", \"Moldova\", \"Monaco\", \"Mongolia\",\n        \"Montenegro\", \"Morocco\", \"Mozambique\", \"Myanmar\", \"Namibia\",\n        \"Nauru\", \"Nepal\", \"Netherlands\", \"New Zealand\", \"Nicaragua\",\n        \"Niger\", \"Nigeria\", \"North Korea\", \"North Macedonia\", \"Norway\",\n        \"Oman\", \"Pakistan\", \"Palau\", \"Panama\", \"Papua New Guinea\",\n        \"Paraguay\", \"Peru\", \"Philippines\", \"Poland\", \"Portugal\",\n        \"Qatar\", \"Romania\", \"Russia\", \"Rwanda\", \"Saint Kitts\",\n        \"Saint Lucia\", \"Saint Vincent\", \"Samoa\", \"San Marino\", \"Sao Tome\",\n        \"Saudi Arabia\", \"Senegal\", \"Serbia\", \"Seychelles\", \"Sierra Leone\",\n        \"Singapore\", \"Slovakia\", \"Slovenia\", \"Solomon Islands\", \"Somalia\",\n        \"South Africa\", \"South Korea\", \"South Sudan\", \"Spain\", \"Sri Lanka\",\n        \"Sudan\", \"Suriname\", \"Sweden\", \"Switzerland\", \"Syria\",\n        \"Taiwan\", \"Tajikistan\", \"Tanzania\", \"Thailand\", \"Timor-Leste\",\n        \"Togo\", \"Tonga\", \"Trinidad\", \"Tunisia\", \"Turkey\",\n        \"Turkmenistan\", \"Tuvalu\", \"Uganda\", \"Ukraine\", \"United Arab Emirates\",\n        \"United Kingdom\", \"United States\", \"Uruguay\", \"Uzbekistan\", \"Vanuatu\",\n        \"Vatican City\", \"Venezuela\", \"Vietnam\", \"Yemen\", \"Zambia\", \"Zimbabwe\"\n    ]\n    \n    country_names += [\n        \"USA\", \"UK\", \"UAE\", \"PRC\", \"DPRK\", \n        \"ROK\", \"DRC\", \"U.S.\", \"U.K.\", \"America\",\n        \"Britain\", \"England\", \"Scotland\", \"Wales\", \"Northern Ireland\",\n        \"Hong Kong\", \"Macau\", \"Palestine\", \"Ivory Coast\", \"Czechia\",\n        \"Macedonia\", \"Swaziland\", \"Burma\", \"East Timor\", \"Vatican\"\n    ]\n    text = re.sub(r'\\b(?:the\\s+)?(?:delegation\\s+of\\s+)?(?:representative\\s+of\\s+)?(' + \n                  '|'.join(country_names) + r')\\b', '[COUNTRY]', text, flags=re.IGNORECASE)\n    \n    text = re.sub(r'[^\\w\\s]', ' ', text) \n    text = text.lower()\n\n    text = ' '.join(text.split()).strip()\n    \n    return text[:3000]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:10.928449Z","iopub.execute_input":"2025-07-12T01:30:10.929730Z","iopub.status.idle":"2025-07-12T01:30:10.940020Z","shell.execute_reply.started":"2025-07-12T01:30:10.929706Z","shell.execute_reply":"2025-07-12T01:30:10.939117Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Semantic Segmentation using GPT 4o","metadata":{}},{"cell_type":"code","source":"def get_segments(text: str, max_tokens = 200) -> list[str]:\n    paragraphs = [p for p in text.split('\\n') if p.split()]\n\n    num_seg = max(0, len(text.split())//max_tokens) + 3\n    system =     system = f\"\"\"\n    You will do semantic segmentation of the following text and output the result as a JSON string.\n    Segment this diplomatic text into {num_seg} coherent policy segments.\n    Each segment should focus on a single policy theme (e.g., economic policy, \n    security concerns, human rights, international cooperation).\n    Preserve diplomatic context and policy coherence within each segment.\n    \n    Return the result as JSON with this exact format:\n    {{\"segments\": [\"segment1\", \"segment2\", \"segment3\"]}}\n    \"\"\"\n    print(\"Doing semantic segmentation...\")\n    response = client.chat.completions.create(\n        model='gpt-4o',\n        messages=[\n            {\"role\": \"system\", \"content\": system},\n            {\"role\": \"user\", \"content\": text}\n        ],\n        response_format={\"type\": \"json_object\"}\n    )\n    \n    segments = json.loads(response.choices[0].message.content)[\"segments\"]\n    if not len(paragraphs) == 1:\n        segments.append(paragraphs)\n    segments.append(text)\n    print(f\"Generated {len(segments)} segments. \")\n    return segments","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:10.941021Z","iopub.execute_input":"2025-07-12T01:30:10.941339Z","iopub.status.idle":"2025-07-12T01:30:10.959317Z","shell.execute_reply.started":"2025-07-12T01:30:10.941313Z","shell.execute_reply":"2025-07-12T01:30:10.958582Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\n\napi = user_secrets.get_secret(\"OPENAI_API_KEY\")\nclient = AzureOpenAI(\n    api_key=api,\n    api_version=\"2024-11-01-preview\",\n    azure_endpoint=\"https://swedencentral.api.cognitive.microsoft.com\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:10.960275Z","iopub.execute_input":"2025-07-12T01:30:10.960577Z","iopub.status.idle":"2025-07-12T01:30:11.403580Z","shell.execute_reply.started":"2025-07-12T01:30:10.960551Z","shell.execute_reply":"2025-07-12T01:30:11.402792Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Ensemble Embeddings using text-embedding-3-large & bge-large-en-v1.5","metadata":{}},{"cell_type":"code","source":"def generate_embeddings(texts: list[str]):\n    print(\"Generating embeddings for each segmentation\")\n    stm = SentenceTransformer('BAAI/bge-large-en-v1.5')\n    embeddings = []\n    for i in texts:\n        bge = stm.encode(i)\n        response = client.embeddings.create(\n            model=\"text-embedding-3-large\",\n            input=i\n        )\n        emb1 = normalize([response.data[0].embedding])[0]\n        emb2 = normalize([bge])[0]\n        embedding = np.concatenate([emb1, emb2])\n        embeddings.append(embedding)\n    \n    fin_emb = []\n    \n    for i in range(len(embeddings)):\n        fin_emb.append(list(embeddings[i]))\n    return fin_emb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:11.404418Z","iopub.execute_input":"2025-07-12T01:30:11.404657Z","iopub.status.idle":"2025-07-12T01:30:11.410660Z","shell.execute_reply.started":"2025-07-12T01:30:11.404637Z","shell.execute_reply":"2025-07-12T01:30:11.409872Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def get_organizations(country: str):\n    return [igo for igo, members in organizations.items() if country in members]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:11.413092Z","iopub.execute_input":"2025-07-12T01:30:11.413328Z","iopub.status.idle":"2025-07-12T01:30:11.430915Z","shell.execute_reply.started":"2025-07-12T01:30:11.413308Z","shell.execute_reply":"2025-07-12T01:30:11.429862Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/better-speech-embeddings/country_embeddings_optimized.csv\").transpose()\ndf2 = pd.read_csv(\"/kaggle/input/2-hour-speech-embedding-average-5200-speeches/my_file.csv\")\ndf.columns = df2.columns\ndf.drop('Unnamed: 0', inplace=True)\ndf.index = range(0, 4096)\n\ndf = df.drop(columns=['Holy See (Vatican City State)'])\ndf = df.rename(columns={'Viet Nam': 'Vietnam',\n                        'Brunei Darussalam': 'Brunei',\n                        'Iran, Islamic Republic of': 'Iran',\n                        \"Lao People's Democratic Republic\": 'Laos',\n                        'Türkiye': 'Turkey',\n                        'Syrian Arab Republic': 'Syria',\n                        'Russian Federation': 'Russia',\n                        'Palestine, State of': 'Palestine',\n                        'Korea, Republic of': 'South Korea',\n                        \"Korea, Democratic People's Republic of\": 'North Korea',\n                        'Bolivia, Plurinational State of': 'Bolivia',\n                        'Moldova, Republic of': 'Moldova',\n                        'Tanzania, United Republic of': 'Tanzania',\n                        'Micronesia, Federated States of':'Micronesia',\n                        'Venezuela, Bolivarian Republic of': 'Venezuela',\n                        'Cabo Verde':'Cape Verde',\n                        'Timor-Leste': 'East Timor',\n                        'Congo, The Democratic Republic of the': 'Democratic Republic of the Congo',\n                        'Czechia': 'Czech Republic',\n                        'Congo': 'Republic of the Congo'})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:11.431601Z","iopub.execute_input":"2025-07-12T01:30:11.431878Z","iopub.status.idle":"2025-07-12T01:30:12.728737Z","shell.execute_reply.started":"2025-07-12T01:30:11.431856Z","shell.execute_reply":"2025-07-12T01:30:12.727864Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df = df.drop(columns=['Palestine'])\n#df = df.drop(columns=['Bolivia, Plurinational State of', 'Brunei Darussalam'])\ncountries = list(df.columns)\nspeech_data = []\nfor country in countries:\n    speech_data.append(list(df[country]))\n\nspeech_data = np.array(speech_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:12.729695Z","iopub.execute_input":"2025-07-12T01:30:12.730016Z","iopub.status.idle":"2025-07-12T01:30:13.025552Z","shell.execute_reply.started":"2025-07-12T01:30:12.729990Z","shell.execute_reply":"2025-07-12T01:30:13.024744Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Combining speech embeddings and knowledge based embeddings","metadata":{}},{"cell_type":"code","source":"cl_speeches = pd.read_csv(\"\")\ndataset = cl_speeches.to_dict(orient='records')\n\nclass EmbeddingPairDataset(Dataset):\n    def __init__(self, emb1, emb2, scores):\n        self.emb1 = torch.tensor(emb1, dtype=torch.float32)\n        self.emb2 = torch.tensor(emb2, dtype=torch.float32)\n        self.scores = torch.tensor(scores, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.scores)\n    def __getitem__(self, idx):\n        return self.emb1[idx], self.emb2[idx], self.scores[idx]\n\nclass ProjectionNet(nn.Module):\n    def __init__(self, input_dim, proj_dim=512):\n        super().__init__()\n        self.proj = nn.Sequential(\n            nn.Linear(input_dim, proj_dim)\n            nn.ReLU(),\n            nn.Linear(proj_dim, proj_dim)\n        )\n    def forward(self, x):\n        return F.normalize(self.proj(x), dim=-1)\n\ndef cosine_similarity_loss(x,y,target_sim):\n    cos_sim = F.cosine_similarity(x,y)\n    return F.mse_loss(cos_sim, target_sim)\n\ndataset = EmbeddingPairDataset(dataset)\ndataloader = Dataloader(dataset, batch_size=32, shuffle=True)\n\nmodel = ProjectionNet(4096)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nfor epoch in range(10):\n    total_loss = 0\n    for a,b,score in dataloader:\n        proj_a = model(a)\n        proj_b = model(b)\n\n        loss = cosine_similarity_loss(proj_a, proj_b, score)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * len(score)\n    print(f\"Epoch: {epoch + 1} | Loss: {total_loss/len(dataset):.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\n\nwith torch.no_grad():\n    speech_emb = torch.tensor(speech_data).float() \n    fused_output = model(speech_emb)\n    transformed_speech = fused_output.squeeze(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:13.026442Z","iopub.execute_input":"2025-07-12T01:30:13.026724Z","iopub.status.idle":"2025-07-12T01:30:13.173906Z","shell.execute_reply.started":"2025-07-12T01:30:13.026700Z","shell.execute_reply":"2025-07-12T01:30:13.172719Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/132087363.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mspeech_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeech_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'transformer' is not defined"],"ename":"NameError","evalue":"name 'transformer' is not defined","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"combined_embedding = torch.reshape(torch.tensor(transformed_speech), (512, 193))\n\nnp_array = combined_embedding.detach().cpu().numpy()\nfinal_country_embeddings = pd.DataFrame(np_array, columns=countries)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:13.174541Z","iopub.status.idle":"2025-07-12T01:30:13.174894Z","shell.execute_reply.started":"2025-07-12T01:30:13.174695Z","shell.execute_reply":"2025-07-12T01:30:13.174713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compare_countries(country1, country2):\n    cs = cosine_similarity(\n        np.array(list(final_country_embeddings[country1])).reshape(1, -1),\n        np.array(list(final_country_embeddings[country2])).reshape(1, -1)\n    )\n    return cs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:13.176252Z","iopub.status.idle":"2025-07-12T01:30:13.176657Z","shell.execute_reply.started":"2025-07-12T01:30:13.176477Z","shell.execute_reply":"2025-07-12T01:30:13.176493Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualizing and testing","metadata":{}},{"cell_type":"code","source":"umap_model = umap.UMAP(n_components=5, random_state=42)\nspeech_umap = umap_model.fit_transform(final_country_embeddings)\nor_speech_umap = umap_model.fit_transform(speech_data)\n\nfrom scipy.spatial.distance import pdist, squareform\n\ndef get_distance_matrix(umap_embeddings, countries, sample_size=5):\n    idx = sorted(random.sample(range(len(countries)), sample_size))\n    selected = umap_embeddings[idx]\n    dist_matrix = squareform(pdist(selected, metric='euclidean'))\n    selected_countries = [countries[i] for i in idx]\n    df = pd.DataFrame(dist_matrix, index=selected_countries, columns=selected_countries)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:13.178260Z","iopub.status.idle":"2025-07-12T01:30:13.178635Z","shell.execute_reply.started":"2025-07-12T01:30:13.178456Z","shell.execute_reply":"2025-07-12T01:30:13.178474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10,8))\n\nspeech_umap_df = get_distance_matrix(speech_umap, countries, 10)\nor_speech_umap_df = get_distance_matrix(or_speech_umap, countries, 10)\n\nsns.heatmap(speech_umap_df, annot=True, vmin=0, vmax=None, cmap=\"Blues_r\")\nplt.title('UMAP Distance Between Country Policy Embeddings')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:13.179941Z","iopub.status.idle":"2025-07-12T01:30:13.180265Z","shell.execute_reply.started":"2025-07-12T01:30:13.180109Z","shell.execute_reply":"2025-07-12T01:30:13.180127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.heatmap(graph_umap_df, annot=True, vmin=0, vmax=None, cmap=\"Blues_r\")\nplt.title('UMAP Distance in Knowledge Injection')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:13.181582Z","iopub.status.idle":"2025-07-12T01:30:13.181907Z","shell.execute_reply.started":"2025-07-12T01:30:13.181744Z","shell.execute_reply":"2025-07-12T01:30:13.181757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.heatmap(or_speech_umap_df, annot=True, vmin=0, vmax=None, cmap=\"Blues_r\")\nplt.title('UMAP Distance in Original Speech Embeddings')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:13.182942Z","iopub.status.idle":"2025-07-12T01:30:13.183222Z","shell.execute_reply.started":"2025-07-12T01:30:13.183104Z","shell.execute_reply":"2025-07-12T01:30:13.183116Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_umap_map(umap):\n    distances = []\n    for i in range(len(countries)):\n        dists = [j**2 for j in umap[i]]\n        dists = math.sqrt(sum(dists))\n        distances.append(dists)\n    \n    world_map = pd.DataFrame({'country': countries, 'value': distances})\n    fig = px.choropleth(world_map,\n                    locations=\"country\",         # Can also use 'iso_alpha' (ISO-3 codes)\n                    locationmode=\"country names\",# or \"ISO-3\"\n                    color=\"value\",               # The numeric column to color by\n                    color_continuous_scale=\"Cividis\",  # Or 'Plasma', 'Cividis', etc.\n                    title=\"World Map by Euclidian Distance of graph\")\n    \n    fig.show()\n\nget_umap_map(speech_umap)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:13.184193Z","iopub.status.idle":"2025-07-12T01:30:13.184437Z","shell.execute_reply.started":"2025-07-12T01:30:13.184320Z","shell.execute_reply":"2025-07-12T01:30:13.184337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_umap = get_distance_matrix(or_speech_umap, countries, 190)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:13.185377Z","iopub.status.idle":"2025-07-12T01:30:13.185645Z","shell.execute_reply.started":"2025-07-12T01:30:13.185531Z","shell.execute_reply":"2025-07-12T01:30:13.185543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"original_country_embeddings = pd.DataFrame(speech_data).T\noriginal_country_embeddings.columns = final_country_embeddings.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:13.186583Z","iopub.status.idle":"2025-07-12T01:30:13.186952Z","shell.execute_reply.started":"2025-07-12T01:30:13.186758Z","shell.execute_reply":"2025-07-12T01:30:13.186773Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"umap_projector = umap.UMAP(n_components=2, random_state=42)\n\ndef generate_country_similarity(speech: str, place: str, df):\n    # 1. Preprocess and embed the input speech\n    clean_speech = clean_text(speech)\n    segments = get_segments(clean_speech)\n    segment_embeddings = np.array(generate_embeddings(segments))\n    av_vec = np.mean(segment_embeddings, axis=0)\n    av_vec = np.expand_dims(av_vec, axis=0)  # shape: (1, 4096)\n\n    transformer.eval()\n    with torch.no_grad():\n        speech_emb = torch.tensor(av_vec).float()  # (1, 4096)\n        graph_emb = torch.tensor(graph_data[list(df.columns).index(place)]).float().unsqueeze(0)  # (1, 128)\n        \n        # Fuse embeddings\n        fused_output = transformer(speech_emb, graph_emb)  # shape: (1, 4096) or other\n        fused_speech_vec = fused_output.squeeze(0).cpu().numpy()  # shape: (D,)\n\n    country_names = list(final_country_embeddings.columns)\n    country_matrix = np.stack([final_country_embeddings[c] for c in country_names])\n\n    sims = cosine_similarity([fused_speech_vec], country_matrix)[0]\n    sorted_data = sorted(zip(sims, country_names), key=lambda x: -x[0])\n    print(\"\\n\\nPolicy Alignment Scores (Cosine Similarity: closer to 1 = better alignment)\\n\")\n    for sim, country in sorted_data:\n        if country == place:\n            print(f'\\n\\n-----{country}------\\n\\n')\n            print(f\"{sim:.6f} — {country}\")\n            if sim > 0.85:\n                print(\"You did a great job in policy alignment!!!\")\n            print(\"\\n\\n\")\n        else:\n            print(f\"{sim:.6f} — {country}\")\n\n    country_sim_list = [(country, sim) for sim, country in sorted_data]\n    countries, values = zip(*country_sim_list)\n    sim_df = pd.DataFrame({'country': countries, 'value': values})\n    all_vectors = np.vstack([fused_speech_vec, country_matrix])\n    all_umap = umap_projector.fit_transform(all_vectors)\n    speech_umap = all_umap[0]\n    country_umap = all_umap[1:]\n    fig = px.choropleth(sim_df,\n                        locations=\"country\",\n                        locationmode=\"country names\",\n                        color=\"value\",\n                        color_continuous_scale=\"Viridis\",\n                        projection=\"natural earth\",\n                        title=\"World Map: Policy Alignment via Cosine Similarity\")\n\n    fig.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:13.187894Z","iopub.status.idle":"2025-07-12T01:30:13.188235Z","shell.execute_reply.started":"2025-07-12T01:30:13.188053Z","shell.execute_reply":"2025-07-12T01:30:13.188068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_country_stats(country: str, num: int):\n    distances = full_umap[country].copy()\n    closest_countries = distances[distances > 0].nsmallest(num)\n    print(f\"3 closest countries to {country}: (UMAP Euclidian Distance)\")\n    print(', '.join(closest_countries.keys()))\n    print(\"\\n\")\n    print(f\"Socioeconomic statistics of {country} as of 2023\")\n    country_data = pd.DataFrame(socioeconomic[socioeconomic['countries']==country]).drop(columns=['countries']).rename(columns={'Density\\n(P/Km2)': 'Density (P/Km2)'})\n    print(''.join(f\"   - {column}: {country_data.iloc[0][column]}\\n\" for column in country_data.columns))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:13.189510Z","iopub.status.idle":"2025-07-12T01:30:13.189786Z","shell.execute_reply.started":"2025-07-12T01:30:13.189658Z","shell.execute_reply":"2025-07-12T01:30:13.189673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MUN_speeches = pd.read_csv(\"/kaggle/input/mun-speech-dataset/MUN Speech Testing.csv\")\nindex = random.randint(0, len(MUN_speeches)-1)\nprint(f\"Policy similarities of speech:\\n\\n{MUN_speeches.loc[index]['Speech']} \\n\\nFrom delegate of nation: {MUN_speeches.loc[index]['Country']}\\n\\n\")\nprint(f\"\\nBasic information about {MUN_speeches.loc[index]['Country']}:\\n\")\nprint(f\"{MUN_speeches.loc[index]['Country']} is part of the following organizations\")\nprint(\"\".join(f\"   - {organization}\\n\" for organization in get_organizations(MUN_speeches.loc[index]['Country'])))\n\nget_country_stats(MUN_speeches.loc[index]['Country'], 3)\n\ngenerate_country_similarity(MUN_speeches.loc[index]['Speech'], MUN_speeches.loc[index]['Country'], df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T01:30:13.190628Z","iopub.status.idle":"2025-07-12T01:30:13.190939Z","shell.execute_reply.started":"2025-07-12T01:30:13.190780Z","shell.execute_reply":"2025-07-12T01:30:13.190794Z"}},"outputs":[],"execution_count":null}]}